he core idea is:

Detect "ECS service scaled to zero": Use a CloudWatch Metric Alarm for this.
Associate this alarm with a Route 53 Health Check: Route 53 can consume CloudWatch alarms as a health check source.
Use that Route 53 Health Check for Failover: Your existing failover record would then rely on this.
Here's how to set up an ECS CloudWatch Metric Alarm for when ECS task replicas are set to zero, and then associate that alarm with a Route 53 Health Check.

Strategy: ECS Task Replica Count to Route 53 Health Check
Instead of relying solely on the ALB health check, which might have a slight delay in detecting zero tasks (it needs to attempt health checks and timeout), this approach provides a more immediate signal directly from ECS.

Components Involved:

CloudWatch Metric Alarm: Monitors the RunningTaskCount for your ECS Service.
Route 53 Health Check (Type: CloudWatch Alarm): A health check that is controlled by the state of the CloudWatch alarm.
Route 53 Record Set: Your existing failover record will then reference this new Route 53 health check.
Step-by-Step Setup and Terraform Code
We'll add to your existing Terraform configuration.

1. Identify ECS Metrics:

AWS CloudWatch automatically collects metrics for ECS. The key metric we need is RunningTaskCount.

Namespace: AWS/ECS
Metric Name: RunningTaskCount
Dimensions:
ClusterName: The name of your ECS cluster.
ServiceName: The name of your ECS service.
2. Create a CloudWatch Metric Alarm:

This alarm will transition to ALARM state when the RunningTaskCount goes to 0 (or below 1).

Terraform Code for CloudWatch Alarm (Add this to your main.tf)

Terraform

# Add this to your main.tf file

# --- Variables for ECS Service (add if not already present) ---
variable "ecs_cluster_name" {
  description = "Name of your ECS cluster in the primary region"
  type        = string
  default     = "your-primary-ecs-cluster" # <--- REPLACE THIS
}

variable "ecs_service_name" {
  description = "Name of your ECS service in the primary region"
  type        = string
  default     = "your-primary-api-service" # <--- REPLACE THIS
}

# --- CloudWatch Metric Alarm for Primary ECS Service (RunningTaskCount) ---
resource "aws_cloudwatch_metric_alarm" "primary_ecs_tasks_zero_alarm" {
  provider = aws.primary # Ensure this uses your primary region provider

  alarm_name          = "${var.ecs_service_name}-RunningTasksZero"
  comparison_operator = "LessThanThreshold" # Alarm if metric is less than threshold
  evaluation_periods  = 1                   # Evaluate over 1 period
  metric_name         = "RunningTaskCount"
  namespace           = "AWS/ECS"
  period              = 60                  # Check every 60 seconds (1 minute)
  statistic           = "Minimum"           # We want to know if it *ever* hits 0 in this period
  threshold           = 1                   # If RunningTaskCount drops below 1 (i.e., 0)

  dimensions = {
    ClusterName = var.ecs_cluster_name
    ServiceName = var.ecs_service_name
  }

  alarm_description = "Alarms if ECS service ${var.ecs_service_name} in ${var.ecs_cluster_name} has 0 running tasks."

  # Optional: Actions to take when in ALARM state (e.g., send to SNS topic for notification)
  # alarm_actions = ["arn:aws:sns:us-east-1:123456789012:MyNotificationTopic"]
  # ok_actions    = ["arn:aws:sns:us-east-1:123456789012:MyNotificationTopic"]

  tags = {
    Name        = "${var.ecs_service_name}-TasksZeroAlarm"
    Environment = "Production"
    Region      = "Primary"
  }
}
Explanation of the CloudWatch Alarm:

comparison_operator = "LessThanThreshold": We want to trigger the alarm if the RunningTaskCount drops below our threshold.
threshold = 1: This means if the RunningTaskCount becomes 0, the alarm will go into ALARM state.
evaluation_periods = 1: The alarm will trigger immediately after one period (60 seconds) if the condition is met.
statistic = "Minimum": If even for a moment within the 60-second period the RunningTaskCount drops to 0, the alarm will fire. Using Average might not catch a brief dip to zero if tasks are rapidly cycling.
3. Create a Route 53 Health Check (Type: CloudWatch Alarm):

This health check's status will mirror the state of the CloudWatch alarm we just created.

Terraform Code for Route 53 Health Check (Add this to your main.tf)

Terraform

# Add this to your main.tf file, alongside your other Route 53 health checks

# --- Route 53 Health Check linked to CloudWatch Alarm for Primary Region ---
resource "aws_route53_health_check" "primary_ecs_zero_health_check" {
  provider = aws.primary # Ensure this uses your primary region provider

  alarm_identifier {
    name    = aws_cloudwatch_metric_alarm.primary_ecs_tasks_zero_alarm.alarm_name
    region  = var.primary_region_alb_zone_id # This should be the actual AWS region name, e.g., "us-east-1"
                                             # Make sure your 'primary_region_alb_zone_id' variable holds the region name, not the zone ID.
                                             # It's better to define a separate variable for region name:
                                             # variable "primary_aws_region_name" { type = string; default = "us-east-1" }
                                             # Then use: region = var.primary_aws_region_name
  }

  type = "CLOUDWATCH_METRIC" # Explicitly define as CloudWatch Metric type

  # No specific endpoint to monitor directly for this type of health check
  # These are not applicable for CLOUDWATCH_METRIC type:
  # ip_address        = null
  # fqdn              = null
  # port              = null
  # resource_path     = null
  # measure_latency   = false
  # request_interval  = null
  # failure_threshold = null
  # enable_sni        = false

  invert_health_check = false # If alarm is INSUFFICIENT_DATA or OK, health check is healthy. If alarm is ALARM, health check is unhealthy.

  tags = {
    Name        = "${var.ecs_service_name}-TasksZero-R53-HC"
    Environment = "Production"
    Region      = "Primary"
  }
}
Important Variable Correction:

In the alarm_identifier block:

region should be the AWS region name (e.g., "us-east-1"), not the ALB's Zone ID.

Recommendation: Add a new variable to your variables.tf (or main.tf) for the primary region name:

Terraform

variable "primary_aws_region_name" {
  description = "The AWS region name for the primary deployment (e.g., us-east-1)"
  type        = string
  default     = "us-east-1" # <--- REPLACE WITH YOUR PRIMARY REGION
}
Then, in the aws_route53_health_check.primary_ecs_zero_health_check resource, use:

Terraform

alarm_identifier {
  name    = aws_cloudwatch_metric_alarm.primary_ecs_tasks_zero_alarm.alarm_name
  region  = var.primary_aws_region_name # Use the specific region name variable
}
4. Update Your Route 53 Failover Record:

Now, you'll modify your primary failover record to include this new health check in a calculated health check or to directly use it.

Option A (Recommended for this scenario): Use a Calculated Health Check (More robust)

A calculated health check allows you to combine multiple health check statuses. This is ideal because you still want the ALB health check to be primary, but the ECS task count provides an additional, immediate signal.

Terraform Code for Calculated Health Check:

Terraform

# Add this to your main.tf file

# --- Route 53 Calculated Health Check (combines ALB health and ECS task count) ---
resource "aws_route53_health_check" "primary_combined_health_check" {
  provider = aws.primary

  type = "CALCULATED"

  child_health_threshold = 1 # At least 1 child health check must be healthy
  child_healthchecks = [
    aws_route53_health_check.primary_api_alb_hc.id,       # Your existing ALB health check
    aws_route53_health_check.primary_ecs_zero_health_check.id # The new ECS tasks zero health check
  ]

  # For CALCULATED health checks:
  #   If the child health checks are healthy, this calculated health check is healthy.
  #   If any of the child health checks become unhealthy, this calculated health check also becomes unhealthy.
  # This provides a stronger signal for failover.

  tags = {
    Name        = "${var.api_subdomain_prefix}-${var.your_main_domain_name}-Combined-HC"
    Environment = "Production"
    Region      = "Primary"
  }
}
Then, update your api_primary_failover_record to use this new calculated health check:

Terraform

resource "aws_route53_record" "api_primary_failover_record" {
  # ... (existing properties) ...

  health_check_id = aws_route53_health_check.primary_combined_health_check.id # <-- UPDATE THIS LINE

  depends_on = [
    aws_route53_health_check.primary_combined_health_check # Update dependency
  ]
}
Option B (Less recommended for this specific scenario): Direct use of ECS Health Check

If you want the ECS task count alarm to be the sole determinant for failover, you would just replace the health_check_id in your api_primary_failover_record with aws_route53_health_check.primary_ecs_zero_health_check.id. However, this bypasses the valuable ALB health checks that detect application-level issues.

Conclusion for Option A (Recommended):

With the calculated health check, your primary region will be marked unhealthy by Route 53 if either:

Your public ALB's health check (primary_api_alb_hc) fails (meaning the API Gateway VPC Endpoint isn't responding, or services behind it are down, including Fargate).
Or, your ECS service's RunningTaskCount goes to zero (primary_ecs_zero_health_check becomes unhealthy).
This provides a more robust and immediate detection mechanism for a "zero replicas" scenario, leading to faster failover.

Final Steps for Terraform:

Review the Entire main.tf: Ensure all variables are correctly defined and all resources are properly referenced.
terraform init: (If you added new providers or modules).
terraform plan: Carefully review the changes. You should see new CloudWatch alarm and Route 53 health check resources, and a modification to your primary Route 53 record.
terraform apply: Confirm and apply the changes.
This setup ensures that a critical state like zero running tasks in your ECS service will quickly trigger a Route 53 failover to your secondary region.
